I".<h2 id="introduction"><strong>Introduction</strong></h2>

<p>해당 프로젝트는 강화학습 알고리즘 중 <strong>DDPG(Deep Deterministic Policy Gradient)</strong>라는 알고리즘을 자율주행 자동차 task에 적용하는 내용으로 진행되었습니다.
본 프로젝트에서 정의하는 문제는, 물리 엔진 (Bullet 2.78)이 장착된 simulator 상에서 차량의 모델이 주어진다고 할 때, lane으로부터의 이탈없이 특정 goal까지 최대한 빠르게 이동하는 것이었습니다.
본 프로젝트에서는 Policy Gradient 기반의 강화학습 알고리즘인 DDPG 알고리즘이 사용되었습니다. Policy Gradient 계열의 강화학습 알고리즘은 objective function의 미분 값을 바탕으로 gradient ascent 기법을 활용하여 가장 큰 return 값을 줄 수 있는 policy를 찾는 최적화 알고리즘입니다.
기존 <strong>DQN(Deep Q Network)</strong> 기법에서는 high-dimensional한 input을 사용할 수는 있었지만, discrete하고 차원이 낮은 action space를 지닌 task만 해결 가능했었습니다. 따라서, DQN에서는 시스템 action의 종류가 늘어날수록 차원의 저주에 걸린다는 문제점이 있었습니다. 이를 해결하고자, Deep Neural Network로 근사화한 action-value function을 사용하는 Model-free, Off-policy Actor-Critic 알고리즘을 제안하였고, 이 알고리즘의 이름을 <strong>Deep Deterministic Policy Gradient(DDPG)</strong>라고 명명했습니다.</p>

<h2 id="task"><strong>Task</strong></h2>

<iframe width="650" height="335" src="https://www.youtube.com/embed/uJyL-RYrtzs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
<p><br /></p>

<h2 id="deep-deterministic-policy-gradientddpg"><strong>Deep Deterministic Policy Gradient(DDPG)</strong></h2>

<ul>
  <li>DDPG는 model-free off-policy actor-critic 알고리즘으로 Deep Q Learning(DQN)과 DPG의 컨셉을 결합한 알고리즘이다. DQN은 discrete한 action space에서 작동되고, DPG는 이를 continuous한 action space로 확장했으며, deterministic한 policy를 사용하였다.</li>
  <li>이 알고리즘이 off-policy인만큼, 이는 actor와 critic이라는 network를 가진다. actor는 explore를 하기 위한 action을 생성한다. actor의 update 과정동안 critic으로부터 생긴 TD error를 사용한다. critic network는 Q-learning의 update rule과 비슷한 TD error를 바탕으로 update가 수행된다.</li>
  <li>Q-learning에서 function approximator로 deep neural network를 사용하므로써 instability issue가 생길 수 있는 것을 확인하였다. 이러한 문제를 해결하기 위해 experience replay와 target networks가 사용되었다.</li>
</ul>

<h2 id="on-policy-learning-vs-off-policy-learning"><strong>On-policy Learning vs Off-policy Learning</strong></h2>

<ul>
  <li>On-policy의 경우, 자신이 직접 시행착오를 겪으면서 스스로 배우는 것에 비유를 할 수 있다. 동일한 policy인 $\pi$에 대하여 sampling된 경험을 따르면서 이를 통해 학습을 하는 방식을 의미한다.</li>
  <li>
    <p>반면, Off-policy의 경우 본인이 아닌 다른이가 시행착오를 겪는 것을 보면서 배우는 것에 비유를 할 수 있다. 다른 policy인 \mu에 대해서 sampling된 경험을 따르면서 자신의 policy인 \pi를 학습하는 방식을 의미한다.</p>
  </li>
  <li>
    <p>DDPG는 high-dimensional이면서 continuous한 action space의 policy를 학습 가능하다는 특징을 가지고 있습니다. DPG와의 비교를 하자면 DDPG는 다음과 같이 표현할 수 있을 것입니다.</p>

    <p>DDPG = DPG + Actor-Critic with DNN</p>
  </li>
  <li>하지만 actor-critic을 그대로 적용하기에는 학습의 converge가 잘 안된다는 단점이 있었고, 이를 해결하고자 DQN의 아이디어를 도입하였습니다.</li>
</ul>

<h2 id="paper"><strong>Paper</strong></h2>

<p><img src="/assets/img/ddpg1.png" width="85%" height="60%" alt="Markdown Monster icon" style="float: center;" /></p>

<p><img src="/assets/img/ddpg2.png" width="85%" height="60%" alt="Markdown Monster icon" style="float: center;" /></p>

<p><br /></p>

<p>For more information please refer to this <a href="https://drive.google.com/file/d/1h8Pxa18lu2fGG2SZx3r15dilqar-Izu7/view?usp=sharing">LINK</a></p>

:ET