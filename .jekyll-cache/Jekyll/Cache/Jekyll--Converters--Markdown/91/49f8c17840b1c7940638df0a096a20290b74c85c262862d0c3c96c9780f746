I"Ï<h2 id="introduction"><strong>Introduction</strong></h2>

<p><strong>Population Ageing is a global phenomenon</strong>. Virtually every country in the world is experiencing growth in the size and proportion of elderly people in their population. There were 703 million persons aged 65+ years in the world in 2019. The number of older persons is projected to double to 1.5 billion in 2050. The percentage of the population aged 65+ years almost doubled from 6% in 1990 to 11% in 2019 in Eastern and South-Eastern Asia, and from 5% in 1990 to 9% in 2019 in Latin America and the Caribbean. [1]</p>

<p><img src="/assets/img/aging_population.png" width="550" height="370" /></p>

<p>To maximize the benefits and manage the risks associated with population ageing, governments and industries should support continuing and lifelong health care and supporting products for all; encourage healthy lifestyles throughout the life course and the education to be familiar with the supporting products such as elder-care robots and household robots.</p>

<p>As a part of this, I think that making robots that can help us in our daily lives is essential to manage and support elderly people in the first place. When we think our home, there are many clutters in the space such as desks, TV, sofa, chair etc. In this workspace, the robot should be able to perceive that environment, avoid human and obstalces while navigating and be able to pick and place a certain objects.</p>

<p float="center">
  <img src="/assets/img/handy.jpg" width="45%" />
  <img src="/assets/img/household_robot.png" width="45%" /> 
</p>

<p>Motivated by the desire to efficiently search for and retrieve fully occluded objects, researchers have considered a robotic system that fully integrates <strong>Radio Frequency (RF)</strong> and <strong>Depth Map</strong> to grasp and retrieve items in line-of-sight, non-line of-sight, and fully occluded settings [2]. This design in robotic grasping problem introduces primitives for RF-visual sensing and learning where neither RF nor vision alone would be efficient.</p>

<p><strong>RFusion</strong> [2], a robotic system that can search for and retrieve items in line-of-sight, non-line-of-sight, and fully occluded settings is consisted of a robotic arm that has a camera and antenna strapped around its gripper, and it uses both of them to find and retrieve target items.  The robot introduces two new primitives: RF-visual sensing and RF-visual reinforcement learning to efficiently localize, maneuver toward, and grasp target items.</p>

<p>It is very accurate. It localizes target items with centimeter-scale precision and achieves 96% success rate in retrieving fully occluded objects, even if they are under a pile. Thus, it paves the way for novel robotic retrieval tasks in complex environments such as warehouses, manufacturing plants, and smart homes.</p>

<iframe width="650" height="335" src="https://www.youtube.com/embed/iqehzw_aLc0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
<p><br /></p>

<p>The objective for my work is to start from implementing RFusionâ€™s work and learn to implement a robotic system that fuses multi-modal datas from different sensors to accomplish a pick-and-place tasks in a cluttered environment where the target item can be hidden under a pile. In particular, I want to expand the previous work with the language-based method because in reality, it is really important to conviniently interact with the robot and the natural language is a common tool that we use everyday. Also, people with disabilities or elders might not be able to interact physically, so this language-based method is worthy to be considered.<br /><br /></p>

<p float="center">  
  <img src="/assets/img/language-based.png" width="95%" />
</p>

<p>The work in [3] introduces a method for incorporating unstructured natural language into Imitation Learning (IL) to teach motor skills to robots. They presented an approach for end-to-end Imitation Learning of robot manipulation policies which combined <strong>language</strong>, <strong>vision</strong>, and <strong>control</strong>. I was really impressed by their ablation studies consisting of the following.</p>

<p float="center">  
  <img src="/assets/img/ablation.png" width="95%" />
</p>

<p>However, they have used FRCNN for perception and GloVe for language embeddings which are quite outdated and more recent models for vision and language, such as BERT can easily be used as a replacement. Also, I want to generalize the task with adopting mobile base and request the robot agent to retrieve target object under a pile with natural language and I think this will be extremely challenging at this moment.</p>

<p float="center">  
  <img src="/assets/img/mobile-ur5.png" width="45%" />
  <img src="/assets/img/slam.png" width="40%" />
</p>

<h2 id="progress"><strong>Progress</strong></h2>

<p><img width="50%" height="50%" src="/assets/img/rfusion4.png" alt="london" /></p>

<p>This project is in progress and the work progress is logged in the following <a href="https://www.notion.so/ybkim95/RFusion-01c984c644fe46039fde35e152a2d5df">LINK</a></p>

<h2 id="reference"><strong>Reference</strong></h2>

<p>[1] Word Population Ageing 2019 Highlights, Department of Economic and Social Affairs, United Nations (2019)</p>

<p>[2] Boroushaki, T., Perper, I., Nachin, M., Rodriguez, A., &amp; Adib, F. (2021, November). RFusion:
Robotic Grasping via RF-Visual Sensing and Learning. In Proceedings of the 19th ACM
Conference on Embedded Networked Sensor Systems (pp. 192-205).</p>

<p>[3] Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., &amp; Ben Amor, H. (2020). Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural Information Processing Systems, 33, 13139-13150.</p>
:ET