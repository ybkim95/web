I":<h2 id="introduction"><strong>Introduction</strong></h2>

<p><strong>Population Ageing is a global phenomenon</strong>. Virtually every country in the world is experiencing growth in the size and proportion of elderly people in their population. There were 703 million persons aged 65+ years in the world in 2019. The number of older persons is projected to double to 1.5 billion in 2050. The percentage of the population aged 65+ years almost doubled from 6% in 1990 to 11% in 2019 in Eastern and South-Eastern Asia, and from 5% in 1990 to 9% in 2019 in Latin America and the Caribbean. [1]</p>

<p><img src="/assets/img/aging_population.png" width="550" height="370" /></p>

<p>To maximize the benefits and manage the risks associated with population ageing, governments and industries should support continuing and lifelong health care and supporting products for all; encourage healthy lifestyles throughout the life course and the education to be familiar with the supporting products such as elder-care robots and household robots.</p>

<p>As a part of this, I think that making robots that can help us in our daily lives is essential to manage and support elderly people in the first place. When we think our home, there are many clutters in the space such as desks, TV, sofa, chair etc. In this workspace, the robot should be able to perceive that environment, avoid human and obstalces while navigating and be able to pick and place a certain objects.</p>

<p float="center">
  <img src="/assets/img/handy.jpg" width="45%" />
  <img src="/assets/img/household_robot.png" width="45%" /> 
</p>

<p>Motivated by the desire to efficiently search for and retrieve fully occluded objects, researchers have considered a robotic system that fully integrates <strong>Radio Frequency (RF)</strong> and <strong>Depth Map</strong> to grasp and retrieve items in line-of-sight, non-line of-sight, and fully occluded settings [2]. This design in robotic grasping problem introduces primitives for RF-visual sensing and learning where neither RF nor vision alone would be efficient.</p>

<p><strong>RFusion</strong> [2], a robotic system that can search for and retrieve items in line-of-sight, non-line-of-sight, and fully occluded settings is consisted of a robotic arm that has a camera and antenna strapped around its gripper, and it uses both of them to find and retrieve target items.  The robot introduces two new primitives: RF-visual sensing and RF-visual reinforcement learning to efficiently localize, maneuver toward, and grasp target items.</p>

<p>It is very accurate. It localizes target items with centimeter-scale precision and achieves 96% success rate in retrieving fully occluded objects, even if they are under a pile. Thus, it paves the way for novel robotic retrieval tasks in complex environments such as warehouses, manufacturing plants, and smart homes.</p>

<iframe width="650" height="335" src="https://www.youtube.com/embed/iqehzw_aLc0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
<p><br /></p>

<p>The objective for my work is to start from implementing RFusionâ€™s work and learn to implement a robotic system that fuses multi-modal datas from different sensors to accomplish a pick-and-place tasks in a cluttered environment where the target item can be hidden under a pile. But, based on this what I want to particularly expand the previous work is to combine language-based method to the robotic tasks.</p>

<p float="center">  
  <img src="/assets/img/language-based.png" width="85%" />
</p>

<!-- <p float="center">  
  <img src="/assets/img/robotnik1.png" width="55%" />
  <img src="/assets/img/non-prehensile.png" width="35%" /> 
</p> -->

<h2 id="progress"><strong>Progress</strong></h2>

<p><img width="60%" height="60%" src="/assets/img/rfusion4.png" alt="london" /></p>

<p>The work progress is updated in the following <a href="https://www.notion.so/ybkim95/RFusion-01c984c644fe46039fde35e152a2d5df">LINK</a></p>

<h2 id="reference"><strong>Reference</strong></h2>

<p>[1] Word Population Ageing 2019 Highlights, Department of Economic and Social Affairs, United Nations (2019)</p>

<p>[2] Boroushaki, T., Perper, I., Nachin, M., Rodriguez, A., &amp; Adib, F. (2021, November). RFusion:
Robotic Grasping via RF-Visual Sensing and Learning. In Proceedings of the 19th ACM
Conference on Embedded Networked Sensor Systems (pp. 192-205).</p>
:ET